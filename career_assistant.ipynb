{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Career Assistant Agent ‚Äì Your Ultimate Guide to a Career in Generative AI!üöÄ\n",
    "\n",
    "## Modificaciones para usar Groq en lugar de Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones actualizadas\n",
    "from typing import Dict, TypedDict\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq  # Reemplazamos ChatGoogleGenerativeAI\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "from IPython.display import display, Image, Markdown\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variable de entorno para la API de Groq\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar la clave de API de Groq\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY', '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')\n",
    "\n",
    "# Inicializar el modelo Groq (usando LLaMA3 70b)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    temperature=0.5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de Estado y Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n de estado y funciones de utilidad\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    category: str\n",
    "    response: str\n",
    "\n",
    "def trim_conversation(prompt):\n",
    "    \"\"\"Trims conversation history to retain only the latest messages within the limit.\"\"\"\n",
    "    max_messages = 10  # Limit the conversation history to the latest 10 messages\n",
    "    return trim_messages(\n",
    "        prompt,\n",
    "        max_tokens=max_messages,  # Specifies the maximum number of messages allowed\n",
    "        strategy=\"last\",  # Trimming strategy to keep the last messages\n",
    "        token_counter=len,  # Counts tokens/messages using the length of the list\n",
    "        start_on=\"human\",  # Start trimming when reaching the first human message\n",
    "        include_system=True,  # Include system messages in the trimmed history\n",
    "        allow_partial=False,  # Ensures only whole messages are included\n",
    "    )\n",
    "\n",
    "def save_file(data, filename):\n",
    "    \"\"\"Saves data to a markdown file with a timestamped filename.\"\"\"\n",
    "    folder_name = \"Agent_output\"  # Folder to store output files\n",
    "    os.makedirs(folder_name, exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "    \n",
    "    # Generate a timestamped filename for uniqueness\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")  # Format: YYYYMMDDHHMMSS\n",
    "    filename = f\"{filename}_{timestamp}.md\"\n",
    "    \n",
    "    # Define the full file path\n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    \n",
    "    # Save the data to the file in the specified path\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(data)\n",
    "        print(f\"File '{file_path}' created successfully.\")\n",
    "    \n",
    "    # Return the full path of the saved file\n",
    "    return file_path\n",
    "\n",
    "def show_md_file(file_path):\n",
    "    \"\"\"Displays the content of a markdown file as Markdown in the notebook.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Render the content in Markdown format within the notebook\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase de Recursos de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningResourceAgent:\n",
    "    def __init__(self, prompt):\n",
    "        # Inicializar el modelo de chat con Groq\n",
    "        self.model = ChatGroq(model=\"llama3-70b-8192\")\n",
    "        self.prompt = prompt\n",
    "        self.tools = [DuckDuckGoSearchResults()]\n",
    "\n",
    "    def TutorialAgent(self, user_input):\n",
    "        # Configurar un agente con acceso a herramientas y ejecutar una respuesta de tutorial\n",
    "        agent = create_tool_calling_agent(self.model, self.tools, self.prompt)\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
    "        response = agent_executor.invoke({\"input\": user_input})\n",
    "        \n",
    "        # Guardar y mostrar la respuesta como archivo markdown\n",
    "        path = save_file(str(response.get('output')).replace(\"```markdown\", \"\").strip(), 'Tutorial')\n",
    "        print(f\"Tutorial guardado en {path}\")\n",
    "        return path\n",
    "\n",
    "    def QueryBot(self, user_input):\n",
    "        # Iniciar un bucle de preguntas y respuestas para interacci√≥n continua con el usuario\n",
    "        print(\"\\nIniciando la sesi√≥n de preguntas y respuestas. Escribe 'exit' para terminar la sesi√≥n.\\n\")\n",
    "        record_QA_session = []\n",
    "        record_QA_session.append(f'Consulta del usuario: {user_input} \\n')\n",
    "        self.prompt.append(HumanMessage(content=user_input))\n",
    "\n",
    "        while True:\n",
    "            # Recortar el historial de conversaci√≥n para mantener el tama√±o del prompt\n",
    "            self.prompt = trim_conversation(self.prompt)\n",
    "            \n",
    "            # Generar una respuesta del modelo de IA y actualizar el historial de conversaci√≥n\n",
    "            response = self.model.invoke(self.prompt)\n",
    "            record_QA_session.append(f'\\nRespuesta del experto: {response.content} \\n')\n",
    "            \n",
    "            self.prompt.append(AIMessage(content=response.content))\n",
    "            \n",
    "            # Mostrar la respuesta de la IA y solicitar entrada del usuario\n",
    "            print('*' * 50 + 'AGENTE' + '*' * 50)\n",
    "            print(\"\\nRESPUESTA DEL AGENTE EXPERTO:\", response.content)\n",
    "            \n",
    "            print('*' * 50 + 'USUARIO' + '*' * 50)\n",
    "            user_input = input(\"\\nSU CONSULTA: \")\n",
    "            record_QA_session.append(f'\\nConsulta del usuario: {user_input} \\n')\n",
    "            self.prompt.append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # Salir del bucle de preguntas y respuestas si el usuario escribe 'exit'\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Terminando la sesi√≥n de chat.\")\n",
    "                path = save_file(''.join(record_QA_session), 'Sesion_Preguntas_Dudas')\n",
    "                print(f\"Sesi√≥n de preguntas guardada en {path}\")\n",
    "                return path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
